{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1206</td><td>application_1573234309149_0939</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop33:8088/proxy/application_1573234309149_0939/\">Link</a></td><td><a target=\"_blank\" href=\"http://gpu2:8042/node/containerlogs/container_e14_1573234309149_0939_01_000001/HH19_Group6__libox964\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "def keras_experiment():\n",
    "    # Imports\n",
    "    from hops import hdfs\n",
    "    from hops import numpy_helper as nph\n",
    "    from hops import experiment\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers as l\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    def log(*args):\n",
    "        hdfs.log(str(args))\n",
    "\n",
    "    # Constants\n",
    "    dataset_path = 'hdfs:///Projects/HH19_Mentors/hh19_dataset/'\n",
    "    num_samples = np.array([1500, 2000, 2500, 3000, 3500, 4000]) # Samples per class\n",
    "    \n",
    "    # Load data\n",
    "    A = nph.load(dataset_path + 'A.npy')\n",
    "\n",
    "    B_train_file = nph.load(dataset_path + 'C_train.npz')\n",
    "    B_train = {n : (B_train_file['images_'+str(n)],\n",
    "                    B_train_file['labels_'+str(n)]) for n in num_samples}\n",
    "\n",
    "    B_test_file = nph.load(dataset_path + 'C_test.npy')\n",
    "    #B_test = (B_test_file['images'],\n",
    "     #         B_test_file['labels'])\n",
    "    \n",
    "    #C_test = nph.load(dataset_path + 'C_test.npy')\n",
    "\n",
    "    print(A.shape)\n",
    "\n",
    "    for num, b in B_train.items():\n",
    "        print(num, b[0].shape, b[1].shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Pre-processing\n",
    "    ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "    ohe.fit(np.arange(10).reshape(-1,1))\n",
    "\n",
    "    B_train_onehot = {n : (b[0], ohe.transform(b[1])) for n,b in B_train.items()}\n",
    "    #t_onehot = B_test[0], ohe.transform(B_test[1])\n",
    "\n",
    "    for num, b in B_train_onehot.items():\n",
    "        print(num, b[1].shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    from tensorflow.keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "    from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras import regularizers\n",
    "    from tensorflow.keras import backend as K\n",
    "\n",
    "    input_img = Input(shape=(32, 32, 3))  # adapt this if using `channels_first` image data format\n",
    "    lambda_val = 0.001\n",
    "\n",
    "    def sparse_reg(activ_matrix):\n",
    "        p = 0.01\n",
    "        beta = 3\n",
    "        p_hat = K.mean(activ_matrix) # average over the batch samples\n",
    "        print(\"p_hat = \",p_hat)\n",
    "        #KLD = p*(K.log(p)-K.log(p_hat)) + (1-p)*(K.log(1-p)-K.log(1-p_hat))\n",
    "        KLD = p*(K.log(p/p_hat)) + (1-p)*(K.log(1-p/1-p_hat))\n",
    "        print(\"KLD = \", KLD)\n",
    "        return beta * K.sum(KLD) # sum over the layer units\n",
    "\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same', \n",
    "               kernel_regularizer=regularizers.l2(lambda_val/2),activity_regularizer=sparse_reg)(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', \n",
    "                     kernel_regularizer=regularizers.l2(lambda_val/2),activity_regularizer=sparse_reg)(x)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    encoder = Model(input_img, encoded)\n",
    "    \n",
    "    A = nph.load(dataset_path + 'A.npy')\n",
    "    A = A.astype('float32')/255.\n",
    "    x_data = A\n",
    "    x_train = x_data[:800,:]\n",
    "    x_test = x_data[800:1000,:]\n",
    "    \n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                epochs=30,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "    \n",
    "    def new_classifier():\n",
    "        model = keras.Sequential()\n",
    "        model.add(l.Conv2D(32, 2, activation='relu', padding='same', input_shape=(4,4,8)))\n",
    "        model.add(l.MaxPool2D(pool_size=4))\n",
    "        model.add(l.Conv2D(32, 2, activation='relu', padding='same'))\n",
    "#         model.add(l.MaxPool2D(pool_size=4))\n",
    "        model.add(l.Flatten())\n",
    "        model.add(l.Dense(64, activation='relu'))\n",
    "        model.add(l.Dense(10, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    log(new_classifier().summary())\n",
    "    \n",
    "    def shuffle_split(list1, list2, split_percent): \n",
    "        combination = list(zip(list1, list2)) \n",
    "        random.shuffle(combination) \n",
    "        list1, list2 = zip(*combination) \n",
    "        list1_train = list1[:int(split_percent*len(list1))] \n",
    "        list1_val = list1[int(split_percent*len(list1)):] \n",
    "        list2_train = list2[:int(split_percent*len(list1))] \n",
    "        list2_val = list2[int(split_percent*len(list1)):] \n",
    "        return np.array(list1_train), np.array(list1_val), np.array(list2_train), np.array(list2_val)\n",
    "\n",
    "    # Train\n",
    "    results ={}\n",
    "    for n in num_samples:\n",
    "        classifier = new_classifier()\n",
    "    #     B_train_decoded = autoencoder_B(B_train_onehot[n])\n",
    "    #     B_train_decoded = autoencoder.predict(B_train_onehot[n][0])\n",
    "        B_decoded = encoder.predict(B_train_onehot[n][0])\n",
    "#         log(B_train_decoded)\n",
    "        B_train_decoded, B_validation_decoded, B_train_label, B_validation_label = shuffle_split(B_decoded, B_train_onehot[n][1],0.9)\n",
    "        \n",
    "        history = classifier.fit(B_train_decoded, B_train_label, batch_size=64, epochs=30,\n",
    "                                 validation_data=(B_validation_decoded,B_validation_label), verbose=False)\n",
    "        \n",
    "        B_test_decoded = encoder.predict(B_test_file)\n",
    "        results[n] = classifier.predict(B_test_decoded, batch_size=64)\n",
    "#         log(np.max(history.history['val_acc']))\n",
    "        print(n, np.max(history.history['val_acc']))\n",
    "\n",
    "    predictions = {'pred_{}'.format(n) : np.argmax(result, axis=-1) for n,result in results.items()}\n",
    "    \n",
    "    log(predictions)\n",
    "    \n",
    "#     # Evaluate\n",
    "#     def accuracy(y_true, y_pred):\n",
    "#         y_true = y_true.reshape(-1)\n",
    "#         y_pred = y_pred.reshape(-1)\n",
    "#         return np.mean(y_true == y_pred)\n",
    "\n",
    "#     accuracies = np.array([accuracy(B_test[1], pred) for n,pred in predictions.items()])\n",
    "#     weights = (0.001*num_samples-0.5)\n",
    "#     score = np.sum(accuracies / weights)\n",
    "\n",
    "#     print(accuracies)\n",
    "#     log(accuracies)\n",
    "#     log(accuracies / weights)\n",
    "#     log('Final score:', score)\n",
    "    \n",
    "    # Save predictions\n",
    "    import os\n",
    "    def savez(hdfs_filename, **data):\n",
    "        local_file = os.path.basename(hdfs_filename)\n",
    "        np.savez(local_file, **data)\n",
    "        hdfs_path = hdfs._expand_path(hdfs_filename, exists=False)\n",
    "        if local_file in hdfs_path:\n",
    "            hdfs_path = hdfs_path.replace(local_file, \"\")\n",
    "        hdfs.copy_to_hdfs(local_file, hdfs_path, overwrite=True)\n",
    "\n",
    "    savez('eval/predictions.npz', **predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "'hdfs://10.0.104.196:8020/Projects/HH19_Group6/Experiments/application_1573234309149_0939/launcher/run.1'\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/hops/anaconda/anaconda/envs/HH19_Group6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])"
     ]
    }
   ],
   "source": [
    "from hops import experiment\n",
    "experiment.launch(keras_experiment, name='test_classifier', local_logdir=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}